errors = [
"""
('HY000', "[HY000] [Simba][ODBC] (10001) General error: org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `users` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 3 pos 5;\n'Project ['id, 'name, 'email, 'age, 'city]\n+- 'Filter ('age < 25)\n   +- 'UnresolvedRelation [users], [], false\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:45)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:352)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:199)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:83)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:67)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:55)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:199)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:194)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:208)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.sql.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `users` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 3 pos 5;\n'Project ['id, 'name, 'email, 'age, 'city]\n+- 'Filter ('age < 25)\n   +- 'UnresolvedRelation [users], [], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:235)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:270)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:269)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:269)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:269)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:269)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:269)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:111)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:183)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:414)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:792)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:414)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:918)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:413)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:99)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:918)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:703)\n\tat org.apache.spark.sql.SparkSession$.withV2WriteAnsiStoreAssignmentPolicy(SparkSession.scala:1442)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:694)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:918)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:694)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:731)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:762)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:653)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$3(SparkExecuteStatementOperation.scala:302)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withRetry(SparkExecuteStatementOperation.scala:441)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:301)\n\t... 16 more\n - SQLState(42P01) - error code(0) (10001) (SQLExecDirectW)")
""",
"""
('HY000', "[HY000] [Simba][ODBC] (10001) General error: org.apache.hive.service.cli.HiveSQLException: Error running query: [UNRESOLVED_COLUMN.WITH_SUGGESTION] org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `site_idd` cannot be resolved. Did you mean one of the following? [`site_id`, `site_name`, `tmzn_id`, `cre_date`, `cre_user`].; line 1 pos 7;\n'GlobalLimit 2\n+- 'LocalLimit 2\n   +- 'Project ['site_idd]\n      +- SubqueryAlias spark_catalog.access_views.dw_sites\n         +- View (`spark_catalog`.`access_views`.`dw_sites`, [site_id#9376170,site_name#9376171,site_domain_code#9376172,dfault_lstg_curncy#9376173,eoa_email_cstmzbl_site_yn_id#9376174,site_cntry_id#9376175,tmzn_id#9376176,cre_date#9376177,upd_date#9376178,cre_user#9376179,upd_user#9376180,short_site_name#9376181])\n            +- Project [site_id#9370203 AS site_id#9376170, site_name#9370204 AS site_name#9376171, site_domain_code#9370205 AS site_domain_code#9376172, dfault_lstg_curncy#9370206 AS dfault_lstg_curncy#9376173, eoa_email_cstmzbl_site_yn_id#9370207 AS eoa_email_cstmzbl_site_yn_id#9376174, site_cntry_id#9370208 AS site_cntry_id#9376175, tmzn_id#9370209 AS tmzn_id#9376176, cre_date#9370210 AS cre_date#9376177, upd_date#9370211 AS upd_date#9376178, cre_user#9370212 AS cre_user#9376179, upd_user#9370213 AS upd_user#9376180, short_site_name#9376169 AS short_site_name#9376181]\n               +- Project [site_id#9370203, site_name#9370204, site_domain_code#9370205, dfault_lstg_curncy#9370206, eoa_email_cstmzbl_site_yn_id#9370207, site_cntry_id#9370208, tmzn_id#9370209, cre_date#9370210, upd_date#9370211, cre_user#9370212, upd_user#9370213, CASE WHEN cast(site_id#9370203 as decimal(10,0)) IN (cast(0 as decimal(10,0)),cast(100 as decimal(10,0))) THEN US WHEN (site_id#9370203 = cast(cast(3 as decimal(1,0)) as decimal(4,0))) THEN UK WHEN (site_id#9370203 = cast(cast(77 as decimal(2,0)) as decimal(4,0))) THEN DE WHEN (site_id#9370203 = cast(cast(15 as decimal(2,0)) as decimal(4,0))) THEN AU WHEN (site_id#9370203 = cast(cast(2 as decimal(1,0)) as decimal(4,0))) THEN CA WHEN (site_id#9370203 = cast(cast(71 as decimal(2,0)) as decimal(4,0))) THEN FR WHEN (site_id#9370203 = cast(cast(101 as decimal(3,0)) as decimal(4,0))) THEN IT WHEN (site_id#9370203 = cast(cast(186 as decimal(3,0)) as decimal(4,0))) THEN ES ELSE ROW END AS short_site_name#9376169]\n                  +- SubqueryAlias spark_catalog.gdw_tables.dw_sites\n                     +- Relation spark_catalog.gdw_tables.dw_sites[site_id#9370203,site_name#9370204,site_domain_code#9370205,dfault_lstg_curncy#9370206,eoa_email_cstmzbl_site_yn_id#9370207,site_cntry_id#9370208,tmzn_id#9370209,cre_date#9370210,upd_date#9370211,cre_user#9370212,upd_user#9370213] parquet\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:45)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:352)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:199)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:83)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:67)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:55)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:199)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:194)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:208)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `site_idd` cannot be resolved. Did you mean one of the following? [`site_id`, `site_name`, `tmzn_id`, `cre_date`, `cre_user`].; line 1 pos 7;\n'GlobalLimit 2\n+- 'LocalLimit 2\n   +- 'Project ['site_idd]\n      +- SubqueryAlias spark_catalog.access_views.dw_sites\n         +- View (`spark_catalog`.`access_views`.`dw_sites`, [site_id#9376170,site_name#9376171,site_domain_code#9376172,dfault_lstg_curncy#9376173,eoa_email_cstmzbl_site_yn_id#9376174,site_cntry_id#9376175,tmzn_id#9376176,cre_date#9376177,upd_date#9376178,cre_user#9376179,upd_user#9376180,short_site_name#9376181])\n            +- Project [site_id#9370203 AS site_id#9376170, site_name#9370204 AS site_name#9376171, site_domain_code#9370205 AS site_domain_code#9376172, dfault_lstg_curncy#9370206 AS dfault_lstg_curncy#9376173, eoa_email_cstmzbl_site_yn_id#9370207 AS eoa_email_cstmzbl_site_yn_id#9376174, site_cntry_id#9370208 AS site_cntry_id#9376175, tmzn_id#9370209 AS tmzn_id#9376176, cre_date#9370210 AS cre_date#9376177, upd_date#9370211 AS upd_date#9376178, cre_user#9370212 AS cre_user#9376179, upd_user#9370213 AS upd_user#9376180, short_site_name#9376169 AS short_site_name#9376181]\n               +- Project [site_id#9370203, site_name#9370204, site_domain_code#9370205, dfault_lstg_curncy#9370206, eoa_email_cstmzbl_site_yn_id#9370207, site_cntry_id#9370208, tmzn_id#9370209, cre_date#9370210, upd_date#9370211, cre_user#9370212, upd_user#9370213, CASE WHEN cast(site_id#9370203 as decimal(10,0)) IN (cast(0 as decimal(10,0)),cast(100 as decimal(10,0))) THEN US WHEN (site_id#9370203 = cast(cast(3 as decimal(1,0)) as decimal(4,0))) THEN UK WHEN (site_id#9370203 = cast(cast(77 as decimal(2,0)) as decimal(4,0))) THEN DE WHEN (site_id#9370203 = cast(cast(15 as decimal(2,0)) as decimal(4,0))) THEN AU WHEN (site_id#9370203 = cast(cast(2 as decimal(1,0)) as decimal(4,0))) THEN CA WHEN (site_id#9370203 = cast(cast(71 as decimal(2,0)) as decimal(4,0))) THEN FR WHEN (site_id#9370203 = cast(cast(101 as decimal(3,0)) as decimal(4,0))) THEN IT WHEN (site_id#9370203 = cast(cast(186 as decimal(3,0)) as decimal(4,0))) THEN ES ELSE ROW END AS short_site_name#9376169]\n                  +- SubqueryAlias spark_catalog.gdw_tables.dw_sites\n                     +- Relation spark_catalog.gdw_tables.dw_sites[site_id#9370203,site_name#9370204,site_domain_code#9370205,dfault_lstg_curncy#9370206,eoa_email_cstmzbl_site_yn_id#9370207,site_cntry_id#9370208,tmzn_id#9370209,cre_date#9370210,upd_date#9370211,cre_user#9370212,upd_user#9370213] parquet\n\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:306)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:141)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:317)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:315)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:270)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:315)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:315)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:315)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:270)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:269)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:269)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:269)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:269)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:269)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:269)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:197)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:193)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:111)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:183)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:414)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:792)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:414)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:918)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:413)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:99)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:918)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:703)\n\tat org.apache.spark.sql.SparkSession$.withV2WriteAnsiStoreAssignmentPolicy(SparkSession.scala:1442)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:694)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:918)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:694)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:731)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:762)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:653)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$3(SparkExecuteStatementOperation.scala:302)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withRetry(SparkExecuteStatementOperation.scala:441)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:301)\n\t... 16 more\n - SQLState(42703) - error code(0) (10001) (SQLExecDirectW)")
""",
"""
('HY000', "[HY000] [Simba][ODBC] (10001) General error: org.apache.hive.service.cli.HiveSQLException: Error running query: [PARSE_SYNTAX_ERROR] org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near ','.(line 8, pos 19)\n\n== SQL ==\nSELECT\n    user_id,\n    COUNT(*) as order_count,\n    SUM(total_amount) as total_spent\nFROM orders\nWHERE order_date >= '2025-01-01'\nGROUP BY user_id\nHAVING COUNT(*) > 5,\n-------------------^^^\nORDER BY total_spent DESC\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:45)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:352)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:199)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:83)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:67)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:55)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:199)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:194)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:208)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near ','.(line 8, pos 19)\n\n== SQL ==\nSELECT\n    user_id,\n    COUNT(*) as order_count,\n    SUM(total_amount) as total_spent\nFROM orders\nWHERE order_date >= '2025-01-01'\nGROUP BY user_id\nHAVING COUNT(*) > 5,\n-------------------^^^\nORDER BY total_spent DESC\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(parsers.scala:257)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:98)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:56)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)\n\tat io.delta.sql.parser.DeltaSqlParser.$anonfun$parsePlan$1(DeltaSqlParser.scala:80)\n\tat io.delta.sql.parser.DeltaSqlParser.parse(DeltaSqlParser.scala:113)\n\tat io.delta.sql.parser.DeltaSqlParser.parsePlan(DeltaSqlParser.scala:77)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$6(SparkSession.scala:696)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:183)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:695)\n\tat org.apache.spark.sql.SparkSession$.withV2WriteAnsiStoreAssignmentPolicy(SparkSession.scala:1442)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:694)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:918)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:694)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:731)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:762)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:653)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$3(SparkExecuteStatementOperation.scala:302)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withRetry(SparkExecuteStatementOperation.scala:441)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:301)\n\t... 16 more\n - SQLState(42601) - error code(0) (10001) (SQLExecDirectW)")
""",
"""
('HY000', '[HY000] [Simba][ODBC] (10001) General error: org.apache.hive.service.cli.HiveSQLException: Error running query: [INVALID_TYPED_LITERAL] org.apache.spark.sql.catalyst.parser.ParseException: \n[INVALID_TYPED_LITERAL] The value of the typed literal "DATE" is invalid: \'2025-s05-15\'.(line 2, pos 16)\n\n== SQL ==\nSELECT *, 1 FROM P_ZETA_AUTOETL_V.PYMT_COP_CK_TRANS_FACT_V2\nWHERE trxn_dt = DATE \'2025-s05-15\'\n----------------^^^\nLIMIT 100\n\n\tat org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:45)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:352)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:199)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:83)\n\tat org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:67)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:55)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:199)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:194)\n\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:208)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.sql.catalyst.parser.ParseException: \n[INVALID_TYPED_LITERAL] The value of the typed literal "DATE" is invalid: \'2025-s05-15\'.(line 2, pos 16)\n\n== SQL ==\nSELECT *, 1 FROM P_ZETA_AUTOETL_V.PYMT_COP_CK_TRANS_FACT_V2\nWHERE trxn_dt = DATE \'2025-s05-15\'\n----------------^^^\nLIMIT 100\n\n\tat org.apache.spark.sql.errors.QueryParsingErrors$.cannotParseValueTypeError(QueryParsingErrors.scala:213)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitTypeConstructor$3(AstBuilder.scala:2766)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.toLiteral$1(AstBuilder.scala:2766)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitTypeConstructor$8(AstBuilder.scala:2780)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitTypeConstructor$1(AstBuilder.scala:2780)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin(SparkParserUtils.scala:125)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin$(SparkParserUtils.scala:115)\n\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:32)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitTypeConstructor(AstBuilder.scala:2760)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitTypeConstructor(AstBuilder.scala:61)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParser$TypeConstructorContext.accept(SqlBaseParser.java:23190)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:116)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParserBaseVisitor.visitConstantDefault(SqlBaseParserBaseVisitor.java:2106)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParser$ConstantDefaultContext.accept(SqlBaseParser.java:21903)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:116)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParserBaseVisitor.visitValueExpressionDefault(SqlBaseParserBaseVisitor.java:1938)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParser$ValueExpressionDefaultContext.accept(SqlBaseParser.java:21011)\n\tat org.apache.spark.sql.catalyst.parser.DataTypeAstBuilder.typedVisit(DataTypeAstBuilder.scala:34)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1966)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitComparison$1(AstBuilder.scala:2074)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin(SparkParserUtils.scala:125)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin$(SparkParserUtils.scala:115)\n\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:32)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitComparison(AstBuilder.scala:2072)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitComparison(AstBuilder.scala:61)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParser$ComparisonContext.accept(SqlBaseParser.java:21038)\n\tat org.apache.spark.sql.catalyst.parser.DataTypeAstBuilder.typedVisit(DataTypeAstBuilder.scala:34)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1966)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitPredicated$1(AstBuilder.scala:2119)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin(SparkParserUtils.scala:125)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin$(SparkParserUtils.scala:115)\n\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:32)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:2118)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitPredicated(AstBuilder.scala:61)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParser$PredicatedContext.accept(SqlBaseParser.java:20429)\n\tat org.apache.spark.sql.catalyst.parser.DataTypeAstBuilder.typedVisit(DataTypeAstBuilder.scala:34)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.expression(AstBuilder.scala:1966)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.withWhereClause(AstBuilder.scala:963)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitCommonSelectQueryClausePlan$2(AstBuilder.scala:1072)\n\tat org.apache.spark.sql.catalyst.parser.package$EnhancedLogicalPlan$.optionalMap$extension(package.scala:42)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitCommonSelectQueryClausePlan(AstBuilder.scala:1072)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$withSelectQuerySpecification$1(AstBuilder.scala:1052)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin(SparkParserUtils.scala:125)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin$(SparkParserUtils.scala:115)\n\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:32)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.withSelectQuerySpecification(AstBuilder.scala:1039)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitRegularQuerySpecification$1(AstBuilder.scala:919)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin(SparkParserUtils.scala:125)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin$(SparkParserUtils.scala:115)\n\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:32)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:906)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitRegularQuerySpecification(AstBuilder.scala:61)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParser$RegularQuerySpecificationContext.accept(SqlBaseParser.java:13511)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:116)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParserBaseVisitor.visitQueryPrimaryDefault(SqlBaseParserBaseVisitor.java:1231)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryPrimaryDefaultContext.accept(SqlBaseParser.java:13000)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:116)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParserBaseVisitor.visitQueryTermDefault(SqlBaseParserBaseVisitor.java:1217)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryTermDefaultContext.accept(SqlBaseParser.java:12767)\n\tat org.apache.spark.sql.catalyst.parser.DataTypeAstBuilder.typedVisit(DataTypeAstBuilder.scala:34)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.plan(AstBuilder.scala:157)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitQuery$1(AstBuilder.scala:163)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin(SparkParserUtils.scala:125)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin$(SparkParserUtils.scala:115)\n\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:32)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:162)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitQuery(AstBuilder.scala:61)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParser$QueryContext.accept(SqlBaseParser.java:8709)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitChildren(AstBuilder.scala:116)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParserBaseVisitor.visitStatementDefault(SqlBaseParserBaseVisitor.java:69)\n\tat org.apache.spark.sql.catalyst.parser.SqlBaseParser$StatementDefaultContext.accept(SqlBaseParser.java:2475)\n\tat org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:18)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitSingleStatement$1(AstBuilder.scala:123)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin(SparkParserUtils.scala:125)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin$(SparkParserUtils.scala:115)\n\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:32)\n\tat org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:123)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.$anonfun$parsePlan$2(AbstractSqlParser.scala:71)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin(SparkParserUtils.scala:125)\n\tat org.apache.spark.sql.catalyst.util.SparkParserUtils.withOrigin$(SparkParserUtils.scala:115)\n\tat org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:32)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.$anonfun$parsePlan$1(AbstractSqlParser.scala:71)\n\tat org.apache.spark.sql.catalyst.parser.AbstractParser.parse(parsers.scala:80)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:56)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(AbstractSqlParser.scala:68)\n\tat io.delta.sql.parser.DeltaSqlParser.$anonfun$parsePlan$1(DeltaSqlParser.scala:80)\n\tat io.delta.sql.parser.DeltaSqlParser.parse(DeltaSqlParser.scala:113)\n\tat io.delta.sql.parser.DeltaSqlParser.parsePlan(DeltaSqlParser.scala:77)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$6(SparkSession.scala:696)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:183)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:695)\n\tat org.apache.spark.sql.SparkSession$.withV2WriteAnsiStoreAssignmentPolicy(SparkSession.scala:1442)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:694)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:918)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:694)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:731)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:762)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:653)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$3(SparkExecuteStatementOperation.scala:302)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withRetry(SparkExecuteStatementOperation.scala:441)\n\tat org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:301)\n\t... 16 more\n - SQLState(42604) - error code(0) (10001) (SQLExecDirectW)')
""",
]